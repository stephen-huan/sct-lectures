\documentclass[11pt, oneside]{article}
\usepackage{titling, hyperref, geometry, amsmath, amssymb, algorithm, graphicx, textcomp, subcaption}
\usepackage[noend]{algpseudocode}
\usepackage[cache=false]{minted}
\geometry{a4paper}

\hypersetup{
  colorlinks=true,
  urlcolor=cyan
}

\title{String Matching}
\author{Stephen Huan}

\begin{document}
\maketitle

\section{Definition}
Let \( T \) be a string of length \( m \) and \( P \) be a string of length \( n \).
We assume both strings contain characters drawn only from a finite alphabet \( \Sigma \),
e.g. \( \Sigma = \{ 0, 1 \} \) for binary and \( \Sigma = \{ A, G, C, T \} \) for DNA.
The \textbf{\textit{string-matching problem}} is defined as finding every index \( i \)
such that \( T[i \dots i + n] = P \), i.e. the substring of \( T \) at that index is
equal to the pattern string \( P \).

We further generalize the problem to include multiple pattern strings, \( P_1, P_2, ... P_k \). We define \( n \) to be \( \Sigma^{k}_{i = 1} |P_i| \) and define the output to be the matches for each pattern string, with a total number of \( z \). Note that \( z \) in the worst case can be
\( O(n^2) \): take the example of \( T = \text{aaa\dots a} \) and \( P_1 = \text{a}, P_2 = \text{aa}, P_3 = \text{aaa}, \dots \)

\begin{table}[h!]
\centering
\begin{tabular}{ ccc }
 Algorithm & Preprocessing time & Matching time \\
 \hline
 Na誰ve & \( 0 \) & \( O(nm) \) \\
 Rabin-Karp & \( \Theta(n) \) & \( O(nm) \) \\
 Finite automaton (na誰ve) & \( \Theta(n|\Sigma|) \) & \( \Theta(m) \) \\
 Knuth-Morris-Pratt (KMP) & \( \Theta(n) \) & \( \Theta(m) \) \\
 Aho-Corasick & \( \Theta(n) \) & \( \Theta(m + z) \) \\
 Suffix Tree & \( \Theta(m) \) & \( \Theta(n + z) \) \\
 Suffix Array & \( \Theta(m) \) & \( O(n + \log m + z) \) \\
 \hline
\end{tabular}
\caption{String-matching algorithms}
\end{table}

A common notation for running time of algorithms is given as \( \langle f, g \rangle \)
where \( f \) is the preprocessing time and \( g \) is the execution time. Using
this notation, Aho-Corasick has a running time of \( \langle O(n), O(m + z) \rangle \).

\newpage

\section{Algorithms}
\subsection{Na誰ve}

\begin{algorithm}
  \begin{algorithmic}[h!]
    \Procedure{Naive-String-Matcher}{$T$, $P$}
      \For{each position in T}
          \For{each pattern string $P_i$}
            \State check for a match
          \EndFor
      \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

For each character in \( T \), in the worst case we go over each \( n \)
characters making the running time \( O(nm) \). Intuitively, this is slow
becuase information about \( T \) doesn't carry over to each string.
For example, if the first character of \( T \) is ``a'', the algorithm should be
able to elimate all patterns which don't start with ``a'' and propagate the check
forward for all patterns which do. How do we implement this ``parallel'' searching in practice?

\begin{figure}[h!]
\centering
\includegraphics[scale=0.25]{trie}
\caption{The trie datastructure}
\end{figure}

Representing the trie is quite simple and can be done in a variety of ways.
One way to represent child pointers is to make a character array of length \( |\Sigma| \),
but that would make construction time \( O(n |\Sigma|) \) as each character would have its own array.
Instead, represent children by a character map, which makes construction \( \Theta(n) \).

To seach with the trie, for each position in \( T \) propagate a pointer starting at the root down the trie as far as possible (until it is no longer in the trie).
Have each node maintain whether it is the end of a pattern word, and on reaching an end node report a match.
This algorithm runs in \( O(m L_\text{max}) \), where \( L_\text{max} \) is the length of the longest pattern string.

\subsection{Suffix Links}

The previous trie algorithm was slow because it restarts at the root for each character.
Instead, we can keep track of nodes to go back to on a ``failure'', or a character mismatch.

A suffix link is defined as a edge between a trie node which represents a string \( \alpha \)
and another trie node which represents \( \omega \) such that \( \omega \)
is the longest proper suffix of \( \alpha \) in the trie and is indicated by a red edge.

Intuitively, on a character mismatch, we can follow suffix links to preserve as much context as possible.
The root corresponds to the empty string \( \epsilon \) and is the only node which doesn't have a suffix link.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.25]{suffix}
\caption{Trie with suffix links shown}
\end{figure}

To search with suffix links, start at the root and for each position in \( T \) propagate a pointer
forward. When there is mismatch, follow suffix links back in the trie until you reach the root
or until you no longer mismatch.

To analyze the running time of this algorithm, note that the most the pointer can go
into the trie is \( m \) forward as it moves exactly one character forward on a match.
In the worst case, each suffix link moves the pointer one back, making the
number of steps falling back at most \( m \), as the suffix links can't move back more than the root.
This bounds the time of the search at \( \Theta(m) \).

\newpage

\subsection{Output Links}

Since suffix links no longer start searching at each position,
certain patterns can be skipped (those which are a suffix of other patterns).
To address this, create output links associated with each node by using
the existing suffix links.
When a node is visited, output all the nodes in the linked list formed by
the output links.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.25]{output}
\caption{Trie with output links shown}
\end{figure}

\subsection{The Final Matching Algorithm}

\begin{itemize}
    \item Start at the root
    \item For each character \textcolor{blue}{c} in the string:.
      \begin{itemize}
        \item While there is no edge labeled \textcolor{blue}{c} and you're not at the root:
          \begin{itemize}
            \item Follow a suffix link
          \end{itemize}
        \item If there's an edge \textcolor{blue}{c}, follow it.
        \item If the current node corresponds to a pattern, output it.
        \item Output all words in the chain of output links originating at this node.
      \end{itemize}
\end{itemize}

\newpage

\subsection{Constructing links}

In order to construct suffix links, we'll use an approach that is basically dynamic programming.
Suppose we have a node that ends with a character \( a \) and therefore represents the word \( wa \).
In order to compute its suffix link, we can use the suffix link of \( w \).
Follow \( w \)'s suffix link to some node \( x \). If \( xa \) exists, then \( wa \) has a suffix link
to \( xa \). Otherwise, follow \( x \)'s suffix link. If you reach the root,
then \( wa \)'s suffix link points to the root.

Suffix links must point from longer strings to shorter strings, becuase a proper suffix of a string
must be shorter than the string. Therefore, maintain the base case of the root having no suffix links
and the root's children's suffix links being the root. Run a breadth-first search on the trie,
and compute the suffix links with the above logic.

The output link of a node \( w \) points to the longest proper suffix of \( w \) that is a pattern,
or null otherwise. Since it points to the \textit{longest} suffix, we can reach all patterns
by a linked list of output links. To construct output links, initialize all of them to null.
While BFS'ing to construct suffix links, use the following logic for a node \( u \).
Let \( v \) be the suffix link of \( u \). If \( v \) is a pattern, \( u \)'s output link
points to \( v \). Otherwise, \( u \)'s output link points to the output link of \( v \).

The analysis of the efficiency of this algorithm is similar to the analysis of the pattern matching algorithm.
The bound on the number of steps backwards is the number of steps forward, and the number of steps forward is
exactly \( n \), making the running-time of the algorithm to generate suffix and output links \( O(n) \).

The resulting graph is known as a ``Aho-Corasick automaton'' beacuse it is a specific way to construct
a finite state automataton.

A sample implementation of this algorithm is given \href{https://gist.github.com/stephen-huan/aa609965c86d750736398c28b025f9be#matching}{here}.

\section{Comparision to Other Algorithms}

Obviously, Aho-Corasick dominates the na誰ve algorithm and Knuth-Morris-Pratt is a specific instance of
Aho-Corasick for one pattern word (the trie becomes a linked list and can be represented by an array).
Some may find Rabin-Karp easier to program but it is mathematically motivated, uses hashing,
and in the worst case could be very slow.

Use Aho-Corasick for when you have set pattern strings and a changing target string,
and use suffix trees/arrays for when you have a set string but changing patterns.

\newpage

\section{Sample Problems}

\begin{enumerate}
  \item \href{http://www.usaco.org/index.php?page=viewproblem2&cpid=533}{USACO Censoring}:
  Given a string \( s \) with length at most \( 10^5 \) and a list of pattern words such that no pattern
  is a substring of another pattern and the total length is at most \( 10^5 \), create a new word
  by repeatedly finding the first instance of a pattern in \( s \) and removing it.

  Solution: Construct an Aho-Corasick automaton from the patterns and scan \( s \) from left to right, adding
  characters of \( s \) to a stack. On a match, remove the length of the matched word from the stack.
  Then, restore the state of the automaton by maintaining an table mapping an index to a trie node.
  Use this table in order to go back to the last character after removing the matched word.

  Recall that in our original formulation for the Aho-Corasick string matching algorithm,
  we allowed backtracking because it would be bounded by the amount it moved forward.
  Since we jump to the last state after removing a word, we no longer have a linear bound
  on the amount of backtracking. Therefore, you will need to also maintain another transition table,
  caching the transition from a trie node given a new character.

  \item \href{https://onlinejudge.org/index.php?option=onlinejudge&page=show_problem&problem=1620}{UVa 10679: I Love Strings!!}:
  Given a target string of length \( \leq 10^5 \) and at most 1,000 pattern strings of length at most 1,000, say for each pattern
  whether or not it's a substring of the target string.

  The obvious thing to do is to use Aho-Corasick on the patterns then run it over the target string.
  There are only three complications:
  \begin{enumerate}
    \item They consider the empty string to not be in the target string.
    \item There can be duplicated words. To address this, maintain a list of words for each trie node.
    When matching a word, add the match to every word in the node's list.
    \item In the worst case of the target string aaa...aaa \( 10^5 \) long and each of the \( 10^3 \) patterns is ``a''
    there are \( 10^3 \cdot 10^5 \) matches which will not run in time. However, note that we only need to report
    whether a pattern is a substring, e.g. it appears 0 or \( \geq 1 \) times. Maintain a set of seen nodes
    and if a node is seen, skip traversing the node's output links. Also maintain a set of seen indexes,
    and if an index is seen, skip adding matches to the rest of the items in the same list.
    Both sets guarentee that each pattern is only matched once, bounding the total complexity to \( O(m) \) instead
    of \( O(m + z) \) and allows the algorithm to run in time.
  \end{enumerate}

  Note that there is a more optimal way to keep track of duplicate words
  (Think of the extreme example I gave previously - if you wanted the number of matches,
  then you could match just one of the words and copy the answer for the duplicates).
  Still maintain a list of words for each trie node, however only update the first pattern's count.
  At the end, run a DFS and for each other pattern in the list set its matches to the first pattern in the same list.

  \item \href{https://contest.usaco.org/DEC05.htm}{USACO 2005 Cow Patterns} (\href{http://poj.org/problem?id=3167}{POJ link}): Farmer John has lined up his \( N \) (\( 1 \leq N \leq 10^5 \)) cows,
  looking for a particular substring of \( K \) (\( 1 \leq K \leq 25,000 \)) cows.
  Each cow has some number of spots S (\( 1 \leq S \leq 25 \)), but he does
  not remember the exact numbers of the \( K \) cows, only their relative ranking. For example, he might
  remember the sequence 1 4 4 3 2 1. This means cows 1 and 6 had the same numbers, as did 2 and 3,
  and 2 had more spots than 4, who had more spots than 5.
  Find the number substrings of the \( N \) cows consistent with the \( K \)-long ranking given.

  \href{https://contest.usaco.org/DEC05anal/cpattern.htm}{Solution}: Use a modified KMP.

  \item \href{https://codeforces.com/problemset/problem/126/B}{Codeforces 126B}: Asterix, Obelix, and their new friends Prefix and Suffix
  want to find a special substring \( t \) of larger string \( s \) (\(1 \leq |s| \leq 10^6 \)).
  This must be the longest substring that appears as a prefix, as a suffix, and as neither.
  Print one such substring or say that none exists.

  Solution: I have a LCP-array-theoretic solution which runs in \( O(m) \) but sadly suffix array construction is too slow. I'm told this uses the prefix function of KMP. Figure it out!

  \item (CLRS)/\href{https://www.spoj.com/problems/EC_WORLD}{SPOJ Rotations}: Give a linear time algorithm for determining whether one string is a cyclic rotation of another.

  Solution: First of all, both strings have to have equal length I'll call \( m \).
  Consider generating all \( m \) possible cyclic rotations of one of the strings: the problem
  then becomes detecting whether any of them are equal to the other string. Instead of
  actually generating these strings, represent a cyclic rotation by its starting index.
  The cyclic string is then the string formed by starting at that index, going to the end,
  then wrapping back around until it reaches its starting location. However, simulating
  this out would be slow. The key observation is that the latest cycle would be at the very end of the string
  and has a length of \( m \). Therefore, all cycles must finish within the original string doubled:
  e.g. if the string is ``car'', all of its cycles (``car'', ``arc'', and ``rca'') are substrings of ``carcar''.

  The solution is then to pick a string, add it to itself, then see if the other string is a substring of the doubled string
  and has equal length. Pick whatever linear matching algorithm you want, the simplest is probably Knuth-Morris-Pratt.

\end{enumerate}

\newpage

\section{Past Lectures}

\begin{enumerate}
  \item \href{https://activities.tjhsst.edu/sct/lectures/1617/2017-06-02_Aho_Corasick.pdf}{``Aho-Corasick'' (Neil Thistlethwaite, 2017)}
  \item \href{https://activities.tjhsst.edu/sct/lectures/1516/SCT_Aho_Corasick.pdf}{``Aho-Corasick'' (Lawrence Wang, 2016)}
  \item \href{https://activities.tjhsst.edu/sct/lectures/1718/2017-10-27_String_Searching.pdf}{``String searching'' [KMP and Rabin-Karp] \\ (Larry Wang and Charles Zhao; presented by Mihir Patel, 2017)}
  \item \href{https://activities.tjhsst.edu/sct/lectures/1415/stringmatching_10_3_14.pdf}{``String Matching Algorithms'' [KMP and Rabin-Karp] \\ (Hariank Muthakana and Corwin de Boor, 2014)}
  \item \href{https://activities.tjhsst.edu/sct/lectures/1314/string_matching_11_01_13.pdf}{``String Matching'' [KMP and Rabin-Karp] (Sreenath Are, 2013)}
  \item \href{https://activities.tjhsst.edu/sct/lectures/1112/string.pdf}{``String Algorithms'' [KMP, Z algorithm] (Nick Haliday, 2012)}
  \item \href{https://activities.tjhsst.edu/sct/lectures/1112/strings111811.pdf}{``Cool String Tricks'' [Rabin-Karp and Tries] (Alex Chen, 2011)}
  \item \href{https://activities.tjhsst.edu/sct/lectures/1617/2016-11-11_String_Searching.pdf}{(Broken) String searching (Larry Wang and Charles Zhao, 2016)}
\end{enumerate}

\section{Works Cited}

\begin{enumerate}
  \item \href{http://web.stanford.edu/class/archive/cs/cs166/cs166.1166/lectures/02/Small02.pdf}{Stanford lecture on Aho-Corasick}
  \item \href{https://web.stanford.edu/class/cs97si/10-string-algorithms.pdf}{``String Algorithms''} by Jaehyun Park for CS 97SI at Stanford
  \item \href{https://mitpress.mit.edu/books/introduction-algorithms-third-edition}{\textit{Introduction to Algorithms}} (also known as CLRS)
\end{enumerate}

\end{document}
